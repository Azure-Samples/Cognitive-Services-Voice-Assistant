[Welcome to MVA + DLS Sample](githublink "Enter Github Link to Repo")
===

Overview
===
This branch contains samples for MVA (Multi Voice Assistant) and DLS (Direct Line Speech) implementation in C# UWP. If you are new to UWP visit [Getting Started with UWP](https://docs.microsoft.com/en-us/windows/uwp/get-started/ "Getting Started with UWP"). If you are new to Azure Cognitive Services visit [Getting Started with Azure Cognitive Services](https://azure.microsoft.com/en-us/services/cognitive-services/ "Azure Cognitive Services").
<br>
 This sample demonstrates how to use Keyword Spotting and Custom Wake Words to enable Multi Voice Assistants with Microsoft's Azure Cognitive Services Speech and Speech SDK. These samples should be used as a guiding tool for developers to implement their own solutions using the Speech SDK or their own Speech SDK.

Prerequisites
===
* A subscription key for the Speech service. See [Try the speech service for free](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/get-started).
* A pre-configured bot created using Bot Framework version 4.2 or above. See [here for steps on how to create a bot](https://blog.botframework.com/2018/05/07/build-a-microsoft-bot-framework-bot-with-the-bot-builder-sdk-v4/). The bot would need to subscribe to [Direct Line Speech](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/tutorial-voice-enable-your-bot-speech-sdk) to send and receive voice inputs. 
* A Windows PC with Windows 10 or later, with a working microphone.
* [Microsoft Visual Studio 2017](https://visualstudio.microsoft.com/), Community Edition or higher.
* The **Universal Windows Platform development** workload in Visual Studio. See [Get set up](https://docs.microsoft.com/en-us/windows/uwp/get-started/get-set-up) to get your machine ready for developing UWP Applications.


Samples List
===
Get started implementing your own MVA + DLS Client Application using Azure Cognitive Services. To use the samples provided, clone this GitHub repository using Git.

```
git clone https://github.com/Microsoft/repoName.git
cd repoName
```

Getting Started with Sample
===
### MVA Only
1. > Clone the application
2. > Navigate to C:/ and create a folder called aarlog
3. > Copy the aarconfig.txt file from the solution and paste it into the newly created aarlog folder
4. > In  Visual Studio, Deploy the Solution
5. > Rebuild and Run the sample
### MVA + DLS Sample
1. > Follow the directions in the [MVA Only][MVA Only] section
2. > Follow the [Voice-enable your bot using the Speed SDK Tutorial](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/tutorial-voice-enable-your-bot-speech-sdk) to enable the your bot to use the Direct Line Speech Channel.
3. > Obtain the Speech Subscription Key linked to Direct Line Speech Channel
4. > Build the application and [Run][Run the sample] the application 
5. > ```Speech SDK must be enabled``` <br>
Enter the Speech Key and Speech Region into the respective fields. 
6. >```Creating Custom Wake Words (Optional)``` <br>
Visit [Custom Wake Word](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/speech-devices-sdk-create-kws) for a tutorial on how to create a Speech SDK Keyword. <br>
Insert the .table file generated by the above tutorial into the SDKKeywords folder of the solution.
7. >```Custom Speech Recognition Endpoint (Optional)``` <br>
   > You can train a [Custom Speech Recognition Model](https://speech.microsoft.com/customspeech) to improve speech recognition quality
8. >```Custom Voice ID (Optional)```
   > You can create a [Custom Voice](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/how-to-custom-voice-create-voice) here
```
Note: If using Custom SR and Custom Voice, you must make sure that your 
Azure Speech Subscription Key is linked to these resources and 
the resources are in the same region as the Bot and Speech Subscription Key.
```
Build the sample
===
* By building this sample you will download the Microsoft Cognitive Services Speech SDK. By downloading you acknowledge its license, see Speech SDK license agreement.
* Download the sample code to your development PC.
* Start Microsoft Visual Studio 2017 (or newer) and select File > Open > Project/Solution.
* This sample code is verified as working on Windows Insider Release Build 19025.vb_release_analog.191112-1600 using [Windows SDK 19018](https://www.microsoft.com/en-us/software-download/windowsinsiderpreviewSDK). Any Build or SDK above the specified versions should be compatible. 
* Navigate to the folder containing this sample, and select the solution file contained within it.
*  Set the active solution configuration and platform to the desired values under Build > Configuration Manager: 
   * On a 64-bit Windows installation, choose x64 as active solution platform.
   * On a 32-bit Windows installation, choose x86 as active solution platform.
* Press Ctrl+Shift+B, or select Build > Build Solution.

Run the sample
===
To debug the app and then run it, press F5 or use **Debug > Start Debugging**. To run the app without debugging, press Ctrl+F5 or use **Debug > Start Without Debugging**.

When you see the `Enable microphone` button, click it. Then select the `Speech recognition with microphone` input button and start speaking. The next (up to) 15 seconds of English speech will be sent to the Speech service and transcribed.
1. > Enter the Speech Key, and Region associated with the Bot
    * Ensure that the Speech Key, and any other resource integrations are registered in the same Azure Region
2. > Click Apply Settings to establish connection to the Direct Line Speech Channel using the entered credentials
3. > Select the settings for your application i.e. Speech SDk, Microphone File Capture, SDK Logging, and Second Stage KWS
4. > Activate the application by either saying the registered keyword or clicking Start Listening.

```
This solution has been tested on Windows Insider 19025.vb_release_analog.191112-1600 using Windows SDK 19018. Any Build and/or SDK above the specified versions should be compatible. 
```

``` 
The aarconfig file does not need to be modified in order to run this solution. The AppID value is the important value in the aarconfig file. The remaining values are modified in the LocalSettingsHelper.cs class upon initial application launch.
```

Modifying the sample
===
### Using [custom wake word](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/speech-devices-sdk-create-kws)
1. > Visit the link above to get started
2. > Add the .table file in the SDKKeywords folder
3. > Navigate to C:/arrlog and add your wake word in the arrconfig.txt file

``` When using 1st stage keyword spotting```
* The C:/aarlog/aarconfig.txt does not need to be modified
* You must obtain a whitelisted AppID, GUID, and Keyword Display Name from Microsoft. 
* Navigate to LocalSettingsHelper.cs
1. > Enter the Keyword Display Name in the ConfiguredKeywordDisplayName method
1. > Enter the whitelisted GUID in the ConfiguredKeywordGuid method
2. > Enter the Language Keyword ID (1033 for en-US, 2057 for en-GB) in the ConfiguredKeywordLanguageId method
3. > Enter the bin file name in the ConfiguredKeywordAudioGraphFile method - This file will be copied to the LocalCache folder in the UWP Local AppData Folder after first run

``` After First Run, you may want to verify that the Keyword was correctly registered to the device, to do so: ```
1. > Open Registry Editor
2. >Navigate to             
    * Computer\HKEY_CURRENT_USER\SOFTWARE\MICROSOFT\SPEECH_OneCore\Settings\VoiceActivation\SignalDetectionConfigurations\{AppId}\{LocalGUID}\{LanguageID}\{GUID}
    * The DisplayName field must match the whitelisted keyword display name entered in the LocalSettingsHelper file. If not, edit it to match the entered name.
    * Active and Available Fields must have a value of 1. If not, edit them to Hexadecimal 1.


Error Handling
===
``` Second Stage KWS ```
* If Second Stage Keyword Spotting (Using Speech SDK) is not working, Make sure that SDKKeywords folder and its contents are copied in the AppX folder
* The AppX folder can be found in **[ProjectFolder]/bin/x64/Debug/AppX**
* If they are not copied, change the properties in the solution to Copy if newer for each file in the folder

``` AAR ```
* AAR service must be restarted upon modifying C:/aarlog/aarconfig

``` Second Stage KWS Table file name must match the Keyword Display Name ```

``` Verifying AAR Keyword Registration to Detector ```
1. > Open Registry Editor
2. > Navigate to Computer\HKEY_CURRENT_USER\SOFTWARE\Microsoft\Speech_OneCore\Settings\VoiceActivation\SignalDetectionConfigurations\{AppId}\{LocalGUID}\{LanguageID}\{GUID}
3. > The DisplayName must match the whitelisted keyword display name entered in the Application. If not, Edit it to match the entered name
4. > Active and Available Fields must have a value of 1. If not, Edit it to Hexademical 1

``` If AAR detection is not responding ```
```
Application is not allowed to voice activate. Click here to view settings
```

**Troubleshooting Option 1:**
1. > Open Task Manager
2. > Click on services, find AarSvc_#####
3. > Click Restart
4. > AAR must be running in order for application to voice activate

Common Issues
===
* Ensure that the Bot, Speech Key, and any other resource integrations are registered in the same Azure Region


References
===

* This is the initial landing page for the Azure Cognitive Services Speech Documentation. It is recommended for beginners to read through the sections of interest to understand the capabilites of Azure Cognitive Services Speech Service `=>` [Speech Services Documentation](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/)

* [Speech SDK API reference for C#](https://docs.microsoft.com/en-us/dotnet/api/microsoft.cognitiveservices.speech?view=azure-dotnet)
* To voice enable your bot using Azure Direct Line Speech Channel `=>` [Voice-enable your bot using the Speed SDK Tutorial](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/tutorial-voice-enable-your-bot-speech-sdk)

* Voice-first Virtual Assistants [FAQ's](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/faq-voice-first-virtual-assistants)

* This E-book is a great resource to fill any knowledge gaps as well as build additional knowledge in building Azure Cognitive Services Solutions `=>` [Learning Azure Cognitive Services E-book](https://azure.microsoft.com/en-us/resources/learning-azure-cognitive-services/ "Azure Cognitive Services E-book")

* Speech SDK documenation landing page `=>` [Speech SDK](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/speech-sdk)


* Windows.Media.Audio namespace allows us to access, modify and process audio `=>` [UWP Audio Graphs API](https://docs.microsoft.com/en-us/windows/uwp/audio-video-camera/audio-graphs)

* The Speech.Audio namespace allows the application to access and output audio streams for processing `=>` [Microsoft.Cognitive.Services.Speech.Audio Namespace](https://docs.microsoft.com/en-us/dotnet/api/microsoft.cognitiveservices.speech.audio?view=azure-dotnet)

* Microsoft.CognitiveServices.Speech.Dialog allows us to connect the Direct Line Speech enabled bot to connect to our UWP Application. View the DialogServiceConfig class for implementation methods `=>` [Microsoft.Cognitive.Services.Speech.Dialog Namespace](https://docs.microsoft.com/en-us/dotnet/api/microsoft.cognitiveservices.speech.dialog?view=azure-dotnet)

* [What is UWP?](https://docs.microsoft.com/en-us/windows/uwp/get-started/universal-application-platform-guide)


Copyright (c) Microsoft Corporation. All rights reserved.